{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† 01 - Fundamentos de LLM (Large Language Models)\n",
    "\n",
    "Este notebook te gu√≠a por los conceptos esenciales de los modelos de lenguaje (LLM) y c√≥mo se usan para generar texto.\n",
    "\n",
    "**Objetivos:**\n",
    "- Entender qu√© es un LLM y c√≥mo se usa mediante API.\n",
    "- Explorar los par√°metros de generaci√≥n (temperature, top-p, etc.).\n",
    "- Observar diferencias de comportamiento del modelo seg√∫n el prompt.\n",
    "- Prepararte para usar Amazon Bedrock y Titan m√°s adelante.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Configuraci√≥n inicial\n",
    "\n",
    "Pod√©s usar este notebook con **OpenAI** (requiere API key) o en modo **simulado** si todav√≠a no la ten√©s.  \n",
    "Para OpenAI, cre√° una variable de entorno llamada `OPENAI_API_KEY` o escribila directamente (no recomendable en entornos p√∫blicos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modo simulado activado. No se encontr√≥ API key v√°lida.\n"
     ]
    }
   ],
   "source": [
    "# Si ten√©s una API key de OpenAI, descoment√° y configurala:\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"tu_api_key_aqui\"\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "    test_model = True\n",
    "except Exception as e:\n",
    "    print(\"Modo simulado activado. No se encontr√≥ API key v√°lida.\")\n",
    "    test_model = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Ejemplo b√°sico de generaci√≥n de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Simulaci√≥n] Un LLM es un modelo de IA que aprende patrones del lenguaje humano a partir de grandes vol√∫menes de texto. Puede generar, resumir o traducir contenido con coherencia contextual. Son la base de herramientas como ChatGPT o Claude.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explica en 3 frases qu√© es un modelo de lenguaje grande (LLM).\"\n",
    "\n",
    "if test_model:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "else:\n",
    "    print(\"[Simulaci√≥n] Un LLM es un modelo de IA que aprende patrones del lenguaje humano a partir de grandes vol√∫menes de texto. Puede generar, resumir o traducir contenido con coherencia contextual. Son la base de herramientas como ChatGPT o Claude.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Explorando par√°metros: temperature y top-p\n",
    "\n",
    "- **temperature**: controla la *creatividad* (0 = m√°s precisa, 1 = m√°s aleatoria).  \n",
    "- **top-p**: usa muestreo acumulativo; controla qu√© porcentaje de probabilidad se considera en cada predicci√≥n.\n",
    "\n",
    "Prob√° con diferentes valores y observ√° c√≥mo cambia el tono o la estructura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Simulaci√≥n] A menor temperature, las respuestas ser√°n m√°s t√©cnicas y concisas; a mayor temperature, m√°s creativas y variadas.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Describe la importancia de la IA en el sector salud.\"\n",
    "\n",
    "if test_model:\n",
    "    for temp in [0, 0.5, 1.0]:\n",
    "        print(f\"\\n--- Temperature = {temp} ---\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temp\n",
    "        )\n",
    "        print(response.choices[0].message.content)\n",
    "else:\n",
    "    print(\"[Simulaci√≥n] A menor temperature, las respuestas ser√°n m√°s t√©cnicas y concisas; a mayor temperature, m√°s creativas y variadas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Ejercicio guiado: comparaci√≥n de prompts\n",
    "\n",
    "Observ√° c√≥mo cambia la respuesta dependiendo de **c√≥mo ped√≠s** lo mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Explica qu√© es la computaci√≥n cu√°ntica.\n",
      "\n",
      "[Simulaci√≥n] Respuesta ejemplo para: 'Explica qu√© es la computaci√≥n cu√°ntica.'\n",
      "\n",
      "Prompt: Explica qu√© es la computaci√≥n cu√°ntica como si tu interlocutor fuera un ni√±o de 10 a√±os.\n",
      "\n",
      "[Simulaci√≥n] Respuesta ejemplo para: 'Explica qu√© es la computaci√≥n cu√°ntica como si tu interlocutor fuera un ni√±o de 10 a√±os.'\n",
      "\n",
      "Prompt: Dame una analog√≠a para entender la computaci√≥n cu√°ntica.\n",
      "\n",
      "[Simulaci√≥n] Respuesta ejemplo para: 'Dame una analog√≠a para entender la computaci√≥n cu√°ntica.'\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Explica qu√© es la computaci√≥n cu√°ntica.\",\n",
    "    \"Explica qu√© es la computaci√≥n cu√°ntica como si tu interlocutor fuera un ni√±o de 10 a√±os.\",\n",
    "    \"Dame una analog√≠a para entender la computaci√≥n cu√°ntica.\",\n",
    "]\n",
    "\n",
    "for p in prompts:\n",
    "    print(f\"\\nPrompt: {p}\\n\")\n",
    "    if test_model:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": p}]\n",
    "        )\n",
    "        print(response.choices[0].message.content)\n",
    "    else:\n",
    "        print(f\"[Simulaci√≥n] Respuesta ejemplo para: '{p}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Reflexi√≥n\n",
    "- Los LLM no *entienden* el lenguaje como humanos, sino que predicen patrones estad√≠sticos.\n",
    "- El dise√±o del **prompt** determina la calidad y precisi√≥n de la respuesta.\n",
    "- Cambiar temperature/top-p afecta el balance entre coherencia y creatividad.\n",
    "\n",
    "üëâ En el pr√≥ximo notebook (`02_rag_example.ipynb`), vas a combinar estas ideas con tus propios documentos (RAG)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
